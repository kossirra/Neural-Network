{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, Embedding, LSTM, Bidirectional\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "n_unique_words = 10000\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=n_unique_words)\n",
        "\n",
        "maxlen = 200\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "def create_model(use_attention=False):\n",
        "    inputs = Input(shape=(maxlen,))\n",
        "    x = Embedding(n_unique_words, 128, input_length=maxlen)(inputs)\n",
        "    x = Bidirectional(LSTM(64, return_sequences=use_attention))(x)\n",
        "    if use_attention:\n",
        "        x = attention()(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=x)\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "class attention(tf.keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super(attention, self).__init__()\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1), initializer=\"normal\")\n",
        "        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1), initializer=\"normal\")\n",
        "        super(attention, self).build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "        e = K.tanh(K.dot(x, self.W) + self.b)\n",
        "        a = K.softmax(e, axis=1)\n",
        "        output = x * a\n",
        "        return K.sum(output, axis=1)\n",
        "\n",
        "model1 = create_model(use_attention=False)\n",
        "model2 = create_model(use_attention=True)\n",
        "\n",
        "history1 = model1.fit(x_train, y_train, batch_size=256, epochs=12, validation_data=(x_test, y_test))\n",
        "history2 = model2.fit(x_train, y_train, batch_size=256, epochs=12, validation_data=(x_test, y_test))\n",
        "\n",
        "print(\"Model 1:\")\n",
        "print(\"Loss: \", history1.history['loss'])\n",
        "print(\"Accuracy: \", history1.history['accuracy'])\n",
        "\n",
        "print(\"Model 2:\")\n",
        "print(\"Loss: \", history2.history['loss'])\n",
        "print(\"Accuracy: \", history2.history['accuracy'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqDoBCj7s4H2",
        "outputId": "8855c6a0-b914-48aa-c77c-76561b6a24fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 1s 0us/step\n",
            "Epoch 1/12\n",
            "98/98 [==============================] - 28s 197ms/step - loss: 0.5055 - accuracy: 0.7385 - val_loss: 0.3245 - val_accuracy: 0.8641\n",
            "Epoch 2/12\n",
            "98/98 [==============================] - 14s 140ms/step - loss: 0.2516 - accuracy: 0.9040 - val_loss: 0.3026 - val_accuracy: 0.8718\n",
            "Epoch 3/12\n",
            "98/98 [==============================] - 11s 114ms/step - loss: 0.1802 - accuracy: 0.9360 - val_loss: 0.3329 - val_accuracy: 0.8692\n",
            "Epoch 4/12\n",
            "98/98 [==============================] - 9s 95ms/step - loss: 0.1397 - accuracy: 0.9503 - val_loss: 0.3611 - val_accuracy: 0.8616\n",
            "Epoch 5/12\n",
            "98/98 [==============================] - 8s 80ms/step - loss: 0.1129 - accuracy: 0.9612 - val_loss: 0.4429 - val_accuracy: 0.8599\n",
            "Epoch 6/12\n",
            "98/98 [==============================] - 8s 80ms/step - loss: 0.0885 - accuracy: 0.9708 - val_loss: 0.5124 - val_accuracy: 0.8556\n",
            "Epoch 7/12\n",
            "98/98 [==============================] - 8s 85ms/step - loss: 0.0852 - accuracy: 0.9726 - val_loss: 0.5530 - val_accuracy: 0.8491\n",
            "Epoch 8/12\n",
            "98/98 [==============================] - 5s 53ms/step - loss: 0.0775 - accuracy: 0.9752 - val_loss: 0.5582 - val_accuracy: 0.8550\n",
            "Epoch 9/12\n",
            "98/98 [==============================] - 7s 69ms/step - loss: 0.0531 - accuracy: 0.9843 - val_loss: 0.5900 - val_accuracy: 0.8432\n",
            "Epoch 10/12\n",
            "98/98 [==============================] - 5s 53ms/step - loss: 0.0551 - accuracy: 0.9816 - val_loss: 0.5534 - val_accuracy: 0.8512\n",
            "Epoch 11/12\n",
            "98/98 [==============================] - 5s 55ms/step - loss: 0.0352 - accuracy: 0.9900 - val_loss: 0.6827 - val_accuracy: 0.8445\n",
            "Epoch 12/12\n",
            "98/98 [==============================] - 6s 58ms/step - loss: 0.0441 - accuracy: 0.9877 - val_loss: 0.6104 - val_accuracy: 0.8431\n",
            "Epoch 1/12\n",
            "98/98 [==============================] - 19s 155ms/step - loss: 0.4934 - accuracy: 0.7563 - val_loss: 0.3746 - val_accuracy: 0.8357\n",
            "Epoch 2/12\n",
            "98/98 [==============================] - 12s 122ms/step - loss: 0.2579 - accuracy: 0.8993 - val_loss: 0.3416 - val_accuracy: 0.8576\n",
            "Epoch 3/12\n",
            "98/98 [==============================] - 10s 103ms/step - loss: 0.1931 - accuracy: 0.9273 - val_loss: 0.3573 - val_accuracy: 0.8607\n",
            "Epoch 4/12\n",
            "98/98 [==============================] - 8s 79ms/step - loss: 0.1562 - accuracy: 0.9440 - val_loss: 0.3342 - val_accuracy: 0.8631\n",
            "Epoch 5/12\n",
            "98/98 [==============================] - 7s 71ms/step - loss: 0.1296 - accuracy: 0.9540 - val_loss: 0.3879 - val_accuracy: 0.8579\n",
            "Epoch 6/12\n",
            "98/98 [==============================] - 7s 70ms/step - loss: 0.1171 - accuracy: 0.9572 - val_loss: 0.3575 - val_accuracy: 0.8422\n",
            "Epoch 7/12\n",
            "98/98 [==============================] - 7s 68ms/step - loss: 0.1077 - accuracy: 0.9622 - val_loss: 0.4965 - val_accuracy: 0.8538\n",
            "Epoch 8/12\n",
            "98/98 [==============================] - 6s 58ms/step - loss: 0.0576 - accuracy: 0.9828 - val_loss: 0.4766 - val_accuracy: 0.8560\n",
            "Epoch 9/12\n",
            "98/98 [==============================] - 6s 59ms/step - loss: 0.0361 - accuracy: 0.9904 - val_loss: 0.5606 - val_accuracy: 0.8500\n",
            "Epoch 10/12\n",
            "98/98 [==============================] - 5s 54ms/step - loss: 0.0346 - accuracy: 0.9898 - val_loss: 0.5814 - val_accuracy: 0.8531\n",
            "Epoch 11/12\n",
            "98/98 [==============================] - 5s 54ms/step - loss: 0.0372 - accuracy: 0.9884 - val_loss: 0.6190 - val_accuracy: 0.8522\n",
            "Epoch 12/12\n",
            "98/98 [==============================] - 4s 45ms/step - loss: 0.0260 - accuracy: 0.9928 - val_loss: 0.6711 - val_accuracy: 0.8482\n",
            "Model 1:\n",
            "Loss:  [0.5054673552513123, 0.2516205608844757, 0.18017666041851044, 0.13965541124343872, 0.11290222406387329, 0.08848444372415543, 0.08522757887840271, 0.07747291773557663, 0.0531369186937809, 0.05508551374077797, 0.035243403166532516, 0.04409988224506378]\n",
            "Accuracy:  [0.7384799718856812, 0.9040399789810181, 0.9360399842262268, 0.9503200054168701, 0.9611600041389465, 0.9708399772644043, 0.9726399779319763, 0.9751999974250793, 0.984279990196228, 0.9815999865531921, 0.9900000095367432, 0.9876800179481506]\n",
            "Model 2:\n",
            "Loss:  [0.49343547224998474, 0.25788167119026184, 0.19312547147274017, 0.1561516672372818, 0.1295890361070633, 0.11714243143796921, 0.10768101364374161, 0.05763205885887146, 0.03611169382929802, 0.03456781432032585, 0.03717230632901192, 0.0260456595569849]\n",
            "Accuracy:  [0.7563199996948242, 0.8993200063705444, 0.9272800087928772, 0.9439600110054016, 0.954039990901947, 0.9571599960327148, 0.9621999859809875, 0.9827600121498108, 0.9904400110244751, 0.9897599816322327, 0.9883599877357483, 0.9927600026130676]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = model1.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEQUbAiKs4_o",
        "outputId": "402c7c54-9618-46d2-d89a-043688e87d73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 7s 9ms/step - loss: 0.6104 - accuracy: 0.8431\n"
          ]
        }
      ]
    }
  ]
}